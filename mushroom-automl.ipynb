{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "from autogluon.common import space\n",
    "from autogluon.core import TabularDataset\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "# auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240804_175220\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 120 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20240804_175220/ds_sub_fit/sub_fit_ho.\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240804_175220/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 14:36:16 UTC 2\n",
      "CPU Count:          16\n",
      "Memory Avail:       19.54 GB / 31.26 GB (62.5%)\n",
      "Disk Space Avail:   461.33 GB / 915.32 GB (50.4%)\n",
      "===================================================\n",
      "Train Data Rows:    2216494\n",
      "Train Data Columns: 21\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21572.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1872.20 MB (8.7% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 8.7% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('int', [])    :  1 | ['id']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('int', [])      :  1 | ['id']\n",
      "\t9.1s = Fit runtime\n",
      "\t21 features in original data used to generate 21 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 103.59 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 10.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 12.99s of the 19.49s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 63.38s compared to 12.79s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsUnif_BAG_L1.\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 8.6s of the 15.1s of remaining time.\n",
      "\tNot enough time to train KNN model on all training rows. Fit 1280000/2216494 rows. (Training KNN model on 2216494 rows is expected to take 8.15s)\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 79.09s compared to 10s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsDist_BAG_L1.\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5.58s of the 12.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.09%)\n",
      "/home/manpm/miniconda3/envs/man/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "\t-0.6592\t = Validation score   (-log_loss)\n",
      "\t6.14s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.5s of the 0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.6592\t = Validation score   (-log_loss)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 19.5s of the -0.66s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.6592\t = Validation score   (-log_loss)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 31.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240804_175220/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                 model  holdout_score  score_val eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1      -0.659104  -0.659248    log_loss        0.445209       0.868307  6.137039                 0.445209                0.868307           6.137039            1       True          1\n",
      "1  WeightedEnsemble_L3      -0.659104  -0.659248    log_loss        0.447195       0.916945  6.189307                 0.001986                0.048637           0.052268            3       True          3\n",
      "2  WeightedEnsemble_L2      -0.659104  -0.659248    log_loss        0.447717       0.927572  6.209080                 0.002508                0.059265           0.072042            2       True          2\n",
      "Stacked overfitting occurred: False.\n",
      "Spend 34 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 86 seconds.\n",
      "Starting full fit now with num_stack_levels 1.\n",
      "Beginning AutoGluon training ... Time limit = 86s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240804_175220\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 14:36:16 UTC 2\n",
      "CPU Count:          16\n",
      "Memory Avail:       19.25 GB / 31.26 GB (61.6%)\n",
      "Disk Space Avail:   461.30 GB / 915.32 GB (50.4%)\n",
      "===================================================\n",
      "Train Data Rows:    2493556\n",
      "Train Data Columns: 21\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21568.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2106.15 MB (9.8% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 9.8% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('int', [])    :  1 | ['id']\n",
      "\t\t('object', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
      "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
      "\t\t('int', [])      :  1 | ['id']\n",
      "\t10.7s = Fit runtime\n",
      "\t21 features in original data used to generate 21 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 116.53 MB (0.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 12.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 49.13s of the 73.71s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 71.39s compared to 59.34s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsUnif_BAG_L1.\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 44.28s of the 68.86s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 68.01s compared to 53.11s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsDist_BAG_L1.\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 39.49s of the 64.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.59%)\n",
      "\t-0.1518\t = Validation score   (-log_loss)\n",
      "\t34.48s\t = Training   runtime\n",
      "\t5.46s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2.25s of the 26.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.64%)\n",
      "\t-0.655\t = Validation score   (-log_loss)\n",
      "\t6.65s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 73.71s of the 17.73s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.1518\t = Validation score   (-log_loss)\n",
      "\t14.25s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting 108 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 3.37s of the 3.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.26%)\n",
      "\t-0.6462\t = Validation score   (-log_loss)\n",
      "\t6.83s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 73.71s of the -6.53s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.1518\t = Validation score   (-log_loss)\n",
      "\t18.72s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 111.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240804_175220\")\n"
     ]
    }
   ],
   "source": [
    "print(\"loading data\")\n",
    "# Prepare data\n",
    "train = pd.read_csv(\"../data/mushrooms/train.csv\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train.drop(columns=\"class\"),\n",
    "    train[\"class\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train[\"class\"],\n",
    ")\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "validate = pd.concat([X_val, y_val], axis=1)\n",
    "test = TabularDataset(\"../data/mushrooms/test.csv\")\n",
    "train = TabularDataset(train)\n",
    "validate = TabularDataset(validate)\n",
    "\n",
    "XGB = {\n",
    "    \"eta\": space.Real(0.01, 0.2, default=0.1),\n",
    "    \"max_depth\": space.Int(\n",
    "        1, 3, default=2\n",
    "    ),  # Adjusted range to be realistic for max_depth\n",
    "    \"min_child_weight\": space.Int(\n",
    "        1, 30, default=1\n",
    "    ),  # Adjusted range to be realistic for min_child_weight\n",
    "    \"gamma\": space.Real(0.01, 0.2, default=0.1),\n",
    "    \"subsample\": space.Real(0.01, 0.2, default=0.1),\n",
    "    \"colsample_bytree\": space.Real(0, 1, default=0.5),\n",
    "    \"lambda\": space.Real(0.01, 0.2, default=0.1),\n",
    "    \"alpha\": space.Real(0.01, 0.2, default=0.1),\n",
    "    \"device\": \"cuda\",\n",
    "    \"enable_categorical\": True,\n",
    "}\n",
    "hyperparameters = {  # hyperparameters of each model type\n",
    "    \"XGB\": XGB,\n",
    "}\n",
    "time_limit = 2 * 60  # train various models for ~2 min\n",
    "num_trials = (\n",
    "    5  # try at most 5 different hyperparameter configurations for each type of model\n",
    ")\n",
    "search_strategy = (\n",
    "    \"auto\"  # to tune hyperparameters using random search routine with a local scheduler\n",
    ")\n",
    "\n",
    "hyperparameter_tune_kwargs = (\n",
    "    {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "        \"num_trials\": num_trials,\n",
    "        \"scheduler\": \"local\",\n",
    "        \"searcher\": search_strategy,\n",
    "    }\n",
    ")  # Refer to TabularPredictor.fit docstring for all valid values\n",
    "# Training\n",
    "predictor = TabularPredictor(\n",
    "    label=\"class\",\n",
    "    problem_type=\"binary\",\n",
    "    eval_metric=\"log_loss\",\n",
    ").fit(\n",
    "    train_data=train,\n",
    "    time_limit=time_limit,\n",
    "    presets=[\"best_quality\"],\n",
    "    # hyperparameters=hyperparameters,\n",
    "    # hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>-0.150638</td>\n",
       "      <td>-0.151805</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.493510</td>\n",
       "      <td>5.462252</td>\n",
       "      <td>34.484753</td>\n",
       "      <td>2.493510</td>\n",
       "      <td>5.462252</td>\n",
       "      <td>34.484753</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-0.150638</td>\n",
       "      <td>-0.151805</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.496814</td>\n",
       "      <td>5.504490</td>\n",
       "      <td>53.202080</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.042238</td>\n",
       "      <td>18.717327</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBMXT_BAG_L2</td>\n",
       "      <td>-0.646025</td>\n",
       "      <td>-0.646154</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>4.417905</td>\n",
       "      <td>7.508646</td>\n",
       "      <td>47.957379</td>\n",
       "      <td>0.981072</td>\n",
       "      <td>1.065474</td>\n",
       "      <td>6.827056</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>-0.654919</td>\n",
       "      <td>-0.654951</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>0.943324</td>\n",
       "      <td>0.980921</td>\n",
       "      <td>6.645570</td>\n",
       "      <td>0.943324</td>\n",
       "      <td>0.980921</td>\n",
       "      <td>6.645570</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.694084</td>\n",
       "      <td>-0.151805</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>2.535895</td>\n",
       "      <td>5.508975</td>\n",
       "      <td>48.735201</td>\n",
       "      <td>0.042386</td>\n",
       "      <td>0.046723</td>\n",
       "      <td>14.250448</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val eval_metric  pred_time_test  \\\n",
       "0    LightGBMXT_BAG_L1   -0.150638  -0.151805    log_loss        2.493510   \n",
       "1  WeightedEnsemble_L3   -0.150638  -0.151805    log_loss        2.496814   \n",
       "2    LightGBMXT_BAG_L2   -0.646025  -0.646154    log_loss        4.417905   \n",
       "3      LightGBM_BAG_L1   -0.654919  -0.654951    log_loss        0.943324   \n",
       "4  WeightedEnsemble_L2   -0.694084  -0.151805    log_loss        2.535895   \n",
       "\n",
       "   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0       5.462252  34.484753                 2.493510                5.462252   \n",
       "1       5.504490  53.202080                 0.003305                0.042238   \n",
       "2       7.508646  47.957379                 0.981072                1.065474   \n",
       "3       0.980921   6.645570                 0.943324                0.980921   \n",
       "4       5.508975  48.735201                 0.042386                0.046723   \n",
       "\n",
       "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0          34.484753            1       True          1  \n",
       "1          18.717327            3       True          5  \n",
       "2           6.827056            2       True          4  \n",
       "3           6.645570            1       True          2  \n",
       "4          14.250448            2       True          3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
