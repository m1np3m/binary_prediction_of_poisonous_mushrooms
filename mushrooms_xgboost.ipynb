{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import gc\n",
    "import optuna\n",
    "from datetime import datetime, timezone\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import joblib as jl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "\n",
    "mlflow.xgboost.autolog()\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "today = datetime.now(timezone.utc).strftime(\"%Y_%m_%d\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# helpers\n",
    "sys.path.append(\"..\")\n",
    "from helpers.loss_functions import *\n",
    "from helpers.mlflow import *\n",
    "\n",
    "# data\n",
    "train_path = \"../data/mushrooms/train.csv\"\n",
    "test_path = \"../data/mushrooms/test.csv\"\n",
    "cache_path = \"../data/mushrooms/cache\"\n",
    "# model\n",
    "is_tunning = True\n",
    "try:\n",
    "    rs = subprocess.check_output(\"nvidia-smi\")\n",
    "    device = \"cuda\" if rs is not None else \"cpu\"\n",
    "    print(f\"device: {device}\")\n",
    "except (\n",
    "    Exception\n",
    "):  # this command not being found can raise quite a few different errors depending on the configuration\n",
    "    print(\"No Nvidia GPU in system!\")\n",
    "    device = \"cpu\"\n",
    "goal = \"binary:logistic\"\n",
    "\n",
    "# custom metric\n",
    "# objective_dict = {\n",
    "#     \"binary:logistic\": {\n",
    "#         \"metric\": {\"is_custom\": True, \"name\": \"MCC\", \"fval\": mcc_metric}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "objective_dict = {\n",
    "    \"binary:logistic\": {\"metric\": {\"is_custom\": False, \"name\": \"logloss\", \"fval\": None}}\n",
    "}\n",
    "metric = objective_dict[goal][\"metric\"][\"name\"]\n",
    "is_custom_metric = objective_dict[goal][\"metric\"][\"is_custom\"]\n",
    "fval = objective_dict[goal][\"metric\"][\"fval\"]\n",
    "best_params = {\n",
    "    \"objective\": goal,\n",
    "    \"device\": device,\n",
    "    \"verbosity\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (3116945, 22)\n",
      "test size: (2077964, 21)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "print(f\"train size: {train.shape}\")\n",
    "test = pd.read_csv(test_path)\n",
    "print(f\"test size: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"class\"\n",
    "\n",
    "categorical_cols = (\n",
    "    train.drop(columns=target).select_dtypes(include=\"object\").columns.to_list()\n",
    ")\n",
    "for c in categorical_cols:\n",
    "    train[c] = train[c].astype(\"category\")\n",
    "    test[c] = test[c].astype(\"category\")\n",
    "numerical_cols = (\n",
    "    train.drop(columns=\"id\").select_dtypes(include=\"number\").columns.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train.drop(columns=target),\n",
    "    train[target],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train[target],\n",
    ")\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create the numerical and categorical pipelines\n",
    "numerical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"num_imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        # (\"minmax\", MinMaxScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_pipeline, numerical_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the full pipeline with the XGBoost model\n",
    "data_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "# Binarize the target labels\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "y_train_binarized = lb.fit_transform(y_train)\n",
    "y_val_binarized = lb.transform(y_val)\n",
    "\n",
    "# prepare data for training\n",
    "dtrain = xgb.DMatrix(X_train_transformed, label=y_train_binarized)\n",
    "dval = xgb.DMatrix(X_val_transformed, label=y_val_binarized)\n",
    "dtest = xgb.DMatrix(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     hyper_parameters = {\n",
    "#         **best_params,\n",
    "#         **{\n",
    "#             \"eta\": trial.suggest_float(\"eta\", 0.01, 0.2),\n",
    "#             \"max_depth\": trial.suggest_int(\"max_depth\", 0.01, 0.03),\n",
    "#             \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0.01, 0.3),\n",
    "#             \"gamma\": trial.suggest_float(\"gamma\", 0.01, 0.2),\n",
    "#             \"subsample\": trial.suggest_float(\"subsample\", 0.01, 0.2),\n",
    "#             \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0, 1),\n",
    "#             \"lambda\": trial.suggest_float(\"lambda\", 0.01, 0.2),\n",
    "#             \"alpha\": trial.suggest_float(\"alpha\", 0.01, 0.2),\n",
    "#         },\n",
    "#     }\n",
    "#     evals_result = {}\n",
    "#     if is_custom_metric:\n",
    "#         xgb.train(\n",
    "#             params=hyper_parameters,\n",
    "#             dtrain=dtrain,\n",
    "#             num_boost_round=10000,\n",
    "#             evals=[(dval, \"eval\")],\n",
    "#             feval=fval,\n",
    "#             evals_result=evals_result,\n",
    "#             early_stopping_rounds=200,\n",
    "#         )\n",
    "#     else:\n",
    "#         xgb.train(\n",
    "#             params=hyper_parameters,\n",
    "#             dtrain=dtrain,\n",
    "#             num_boost_round=10000,\n",
    "#             evals=[(dval, \"eval\")],\n",
    "#             evals_result=evals_result,\n",
    "#             early_stopping_rounds=200,\n",
    "#         )\n",
    "#     return evals_result[\"eval\"][metric][-1]\n",
    "\n",
    "\n",
    "# if is_tunning:\n",
    "#     # Create or load a study\n",
    "#     today = datetime.now(timezone.utc).strftime(\"%Y_%m_%d\")\n",
    "#     curr_timestamp = int(datetime.now(timezone.utc).timestamp())\n",
    "#     study_name = f\"study_{today}\"\n",
    "#     study = optuna.create_study(\n",
    "#         study_name=study_name,\n",
    "#         storage=f\"sqlite:///{study_name}.db\",\n",
    "#         direction=\"minimize\",\n",
    "#         load_if_exists=True,\n",
    "#     )\n",
    "#     study.optimize(objective, n_trials=100, timeout=None, show_progress_bar=True)\n",
    "#     # Print best trial\n",
    "#     best_trial = study.best_trial\n",
    "#     print(\"Best trial:\")\n",
    "#     print(f\" {metric}:\", best_trial.value)\n",
    "#     print(\"  Params: \")\n",
    "#     for key, value in best_trial.params.items():\n",
    "#         print(\"    {}: {}\".format(key, value))\n",
    "#     study_best_params = study.best_params\n",
    "#     best_params.update(study_best_params)\n",
    "#     jl.dump(best_params, \"best_params.pkl\")\n",
    "#     # 0.03734 anh Tu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/05 00:01:15 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: `pynvml` is not installed, to log GPU metrics please run `pip install pynvml` to install it..\n",
      "2024/08/05 00:01:15 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting connection to Mlflow...\n",
      "Checking whether experiment existed\n",
      "Using existed experiment: train-mushroom-classifier\n",
      "Training best model...\n",
      "Training with original metric\n",
      "[0]\teval-logloss:0.56379\n",
      "[1]\teval-logloss:0.47158\n",
      "[2]\teval-logloss:0.40652\n",
      "[3]\teval-logloss:0.35440\n",
      "[4]\teval-logloss:0.30978\n",
      "[5]\teval-logloss:0.27201\n",
      "[6]\teval-logloss:0.24125\n",
      "[7]\teval-logloss:0.21564\n",
      "[8]\teval-logloss:0.19675\n",
      "[9]\teval-logloss:0.17898\n",
      "[10]\teval-logloss:0.15863\n",
      "[11]\teval-logloss:0.14343\n",
      "[12]\teval-logloss:0.12918\n",
      "[13]\teval-logloss:0.11914\n",
      "[14]\teval-logloss:0.10876\n",
      "[15]\teval-logloss:0.10419\n",
      "[16]\teval-logloss:0.09789\n",
      "[17]\teval-logloss:0.09253\n",
      "[18]\teval-logloss:0.08659\n",
      "[19]\teval-logloss:0.08382\n",
      "[20]\teval-logloss:0.08045\n",
      "[21]\teval-logloss:0.07785\n",
      "[22]\teval-logloss:0.07498\n",
      "[23]\teval-logloss:0.07316\n",
      "[24]\teval-logloss:0.07093\n",
      "[25]\teval-logloss:0.06827\n",
      "[26]\teval-logloss:0.06719\n",
      "[27]\teval-logloss:0.06378\n",
      "[28]\teval-logloss:0.06160\n",
      "[29]\teval-logloss:0.05979\n",
      "[30]\teval-logloss:0.05820\n",
      "[31]\teval-logloss:0.05666\n",
      "[32]\teval-logloss:0.05433\n",
      "[33]\teval-logloss:0.05397\n",
      "[34]\teval-logloss:0.05322\n",
      "[35]\teval-logloss:0.05161\n",
      "[36]\teval-logloss:0.05065\n",
      "[37]\teval-logloss:0.04984\n",
      "[38]\teval-logloss:0.04955\n",
      "[39]\teval-logloss:0.04932\n",
      "[40]\teval-logloss:0.04878\n",
      "[41]\teval-logloss:0.04821\n",
      "[42]\teval-logloss:0.04756\n",
      "[43]\teval-logloss:0.04742\n",
      "[44]\teval-logloss:0.04725\n",
      "[45]\teval-logloss:0.04674\n",
      "[46]\teval-logloss:0.04663\n",
      "[47]\teval-logloss:0.04637\n",
      "[48]\teval-logloss:0.04603\n",
      "[49]\teval-logloss:0.04565\n",
      "[50]\teval-logloss:0.04512\n",
      "[51]\teval-logloss:0.04445\n",
      "[52]\teval-logloss:0.04417\n",
      "[53]\teval-logloss:0.04374\n",
      "[54]\teval-logloss:0.04328\n",
      "[55]\teval-logloss:0.04318\n",
      "[56]\teval-logloss:0.04277\n",
      "[57]\teval-logloss:0.04258\n",
      "[58]\teval-logloss:0.04230\n",
      "[59]\teval-logloss:0.04224\n",
      "[60]\teval-logloss:0.04192\n",
      "[61]\teval-logloss:0.04161\n",
      "[62]\teval-logloss:0.04148\n",
      "[63]\teval-logloss:0.04127\n",
      "[64]\teval-logloss:0.04122\n",
      "[65]\teval-logloss:0.04102\n",
      "[66]\teval-logloss:0.04087\n",
      "[67]\teval-logloss:0.04076\n",
      "[68]\teval-logloss:0.04071\n",
      "[69]\teval-logloss:0.04061\n",
      "[70]\teval-logloss:0.04054\n",
      "[71]\teval-logloss:0.04048\n",
      "[72]\teval-logloss:0.04034\n",
      "[73]\teval-logloss:0.04032\n",
      "[74]\teval-logloss:0.04022\n",
      "[75]\teval-logloss:0.04018\n",
      "[76]\teval-logloss:0.04014\n",
      "[77]\teval-logloss:0.04010\n",
      "[78]\teval-logloss:0.04007\n",
      "[79]\teval-logloss:0.03995\n",
      "[80]\teval-logloss:0.03989\n",
      "[81]\teval-logloss:0.03986\n",
      "[82]\teval-logloss:0.03981\n",
      "[83]\teval-logloss:0.03970\n",
      "[84]\teval-logloss:0.03966\n",
      "[85]\teval-logloss:0.03956\n",
      "[86]\teval-logloss:0.03950\n",
      "[87]\teval-logloss:0.03938\n",
      "[88]\teval-logloss:0.03934\n",
      "[89]\teval-logloss:0.03932\n",
      "[90]\teval-logloss:0.03924\n",
      "[91]\teval-logloss:0.03922\n",
      "[92]\teval-logloss:0.03916\n",
      "[93]\teval-logloss:0.03914\n",
      "[94]\teval-logloss:0.03910\n",
      "[95]\teval-logloss:0.03906\n",
      "[96]\teval-logloss:0.03898\n",
      "[97]\teval-logloss:0.03896\n",
      "[98]\teval-logloss:0.03894\n",
      "[99]\teval-logloss:0.03892\n",
      "[100]\teval-logloss:0.03890\n",
      "[101]\teval-logloss:0.03887\n",
      "[102]\teval-logloss:0.03884\n",
      "[103]\teval-logloss:0.03881\n",
      "[104]\teval-logloss:0.03879\n",
      "[105]\teval-logloss:0.03876\n",
      "[106]\teval-logloss:0.03874\n",
      "[107]\teval-logloss:0.03869\n",
      "[108]\teval-logloss:0.03867\n",
      "[109]\teval-logloss:0.03864\n",
      "[110]\teval-logloss:0.03862\n",
      "[111]\teval-logloss:0.03860\n",
      "[112]\teval-logloss:0.03859\n",
      "[113]\teval-logloss:0.03857\n",
      "[114]\teval-logloss:0.03851\n",
      "[115]\teval-logloss:0.03849\n",
      "[116]\teval-logloss:0.03847\n",
      "[117]\teval-logloss:0.03846\n",
      "[118]\teval-logloss:0.03844\n",
      "[119]\teval-logloss:0.03843\n",
      "[120]\teval-logloss:0.03842\n",
      "[121]\teval-logloss:0.03841\n",
      "[122]\teval-logloss:0.03839\n",
      "[123]\teval-logloss:0.03837\n",
      "[124]\teval-logloss:0.03836\n",
      "[125]\teval-logloss:0.03834\n",
      "[126]\teval-logloss:0.03833\n",
      "[127]\teval-logloss:0.03832\n",
      "[128]\teval-logloss:0.03831\n",
      "[129]\teval-logloss:0.03829\n",
      "[130]\teval-logloss:0.03829\n",
      "[131]\teval-logloss:0.03826\n",
      "[132]\teval-logloss:0.03826\n",
      "[133]\teval-logloss:0.03825\n",
      "[134]\teval-logloss:0.03823\n",
      "[135]\teval-logloss:0.03823\n",
      "[136]\teval-logloss:0.03822\n",
      "[137]\teval-logloss:0.03820\n",
      "[138]\teval-logloss:0.03820\n",
      "[139]\teval-logloss:0.03819\n",
      "[140]\teval-logloss:0.03818\n",
      "[141]\teval-logloss:0.03817\n",
      "[142]\teval-logloss:0.03816\n",
      "[143]\teval-logloss:0.03815\n",
      "[144]\teval-logloss:0.03814\n",
      "[145]\teval-logloss:0.03812\n",
      "[146]\teval-logloss:0.03811\n",
      "[147]\teval-logloss:0.03810\n",
      "[148]\teval-logloss:0.03808\n",
      "[149]\teval-logloss:0.03807\n",
      "[150]\teval-logloss:0.03806\n",
      "[151]\teval-logloss:0.03806\n",
      "[152]\teval-logloss:0.03805\n",
      "[153]\teval-logloss:0.03804\n",
      "[154]\teval-logloss:0.03803\n",
      "[155]\teval-logloss:0.03802\n",
      "[156]\teval-logloss:0.03801\n",
      "[157]\teval-logloss:0.03801\n",
      "[158]\teval-logloss:0.03800\n",
      "[159]\teval-logloss:0.03799\n",
      "[160]\teval-logloss:0.03798\n",
      "[161]\teval-logloss:0.03798\n",
      "[162]\teval-logloss:0.03797\n",
      "[163]\teval-logloss:0.03796\n",
      "[164]\teval-logloss:0.03795\n",
      "[165]\teval-logloss:0.03794\n",
      "[166]\teval-logloss:0.03794\n",
      "[167]\teval-logloss:0.03793\n",
      "[168]\teval-logloss:0.03793\n",
      "[169]\teval-logloss:0.03792\n",
      "[170]\teval-logloss:0.03792\n",
      "[171]\teval-logloss:0.03791\n",
      "[172]\teval-logloss:0.03790\n",
      "[173]\teval-logloss:0.03790\n",
      "[174]\teval-logloss:0.03789\n",
      "[175]\teval-logloss:0.03789\n",
      "[176]\teval-logloss:0.03790\n",
      "[177]\teval-logloss:0.03789\n",
      "[178]\teval-logloss:0.03789\n",
      "[179]\teval-logloss:0.03789\n",
      "[180]\teval-logloss:0.03788\n",
      "[181]\teval-logloss:0.03787\n",
      "[182]\teval-logloss:0.03787\n",
      "[183]\teval-logloss:0.03787\n",
      "[184]\teval-logloss:0.03787\n",
      "[185]\teval-logloss:0.03787\n",
      "[186]\teval-logloss:0.03787\n",
      "[187]\teval-logloss:0.03787\n",
      "[188]\teval-logloss:0.03788\n",
      "[189]\teval-logloss:0.03788\n",
      "[190]\teval-logloss:0.03787\n",
      "[191]\teval-logloss:0.03787\n",
      "[192]\teval-logloss:0.03787\n",
      "[193]\teval-logloss:0.03787\n",
      "[194]\teval-logloss:0.03787\n",
      "[195]\teval-logloss:0.03787\n",
      "[196]\teval-logloss:0.03786\n",
      "[197]\teval-logloss:0.03786\n",
      "[198]\teval-logloss:0.03786\n",
      "[199]\teval-logloss:0.03786\n",
      "[200]\teval-logloss:0.03786\n",
      "[201]\teval-logloss:0.03786\n",
      "[202]\teval-logloss:0.03786\n",
      "[203]\teval-logloss:0.03786\n",
      "[204]\teval-logloss:0.03786\n",
      "[205]\teval-logloss:0.03786\n",
      "[206]\teval-logloss:0.03785\n",
      "[207]\teval-logloss:0.03786\n",
      "[208]\teval-logloss:0.03785\n",
      "[209]\teval-logloss:0.03785\n",
      "[210]\teval-logloss:0.03786\n",
      "[211]\teval-logloss:0.03785\n",
      "[212]\teval-logloss:0.03785\n",
      "[213]\teval-logloss:0.03785\n",
      "[214]\teval-logloss:0.03784\n",
      "[215]\teval-logloss:0.03784\n",
      "[216]\teval-logloss:0.03784\n",
      "[217]\teval-logloss:0.03784\n",
      "[218]\teval-logloss:0.03783\n",
      "[219]\teval-logloss:0.03783\n",
      "[220]\teval-logloss:0.03784\n",
      "[221]\teval-logloss:0.03784\n",
      "[222]\teval-logloss:0.03784\n",
      "[223]\teval-logloss:0.03783\n",
      "[224]\teval-logloss:0.03783\n",
      "[225]\teval-logloss:0.03783\n",
      "[226]\teval-logloss:0.03783\n",
      "[227]\teval-logloss:0.03783\n",
      "[228]\teval-logloss:0.03783\n",
      "[229]\teval-logloss:0.03783\n",
      "[230]\teval-logloss:0.03782\n",
      "[231]\teval-logloss:0.03782\n",
      "[232]\teval-logloss:0.03782\n",
      "[233]\teval-logloss:0.03782\n",
      "[234]\teval-logloss:0.03782\n",
      "[235]\teval-logloss:0.03782\n",
      "[236]\teval-logloss:0.03782\n",
      "[237]\teval-logloss:0.03782\n",
      "[238]\teval-logloss:0.03782\n",
      "[239]\teval-logloss:0.03781\n",
      "[240]\teval-logloss:0.03782\n",
      "[241]\teval-logloss:0.03782\n",
      "[242]\teval-logloss:0.03782\n",
      "[243]\teval-logloss:0.03782\n",
      "[244]\teval-logloss:0.03782\n",
      "[245]\teval-logloss:0.03782\n",
      "[246]\teval-logloss:0.03782\n",
      "[247]\teval-logloss:0.03782\n",
      "[248]\teval-logloss:0.03782\n",
      "[249]\teval-logloss:0.03782\n",
      "[250]\teval-logloss:0.03782\n",
      "[251]\teval-logloss:0.03782\n",
      "[252]\teval-logloss:0.03782\n",
      "[253]\teval-logloss:0.03781\n",
      "[254]\teval-logloss:0.03781\n",
      "[255]\teval-logloss:0.03781\n",
      "[256]\teval-logloss:0.03781\n",
      "[257]\teval-logloss:0.03781\n",
      "[258]\teval-logloss:0.03781\n",
      "[259]\teval-logloss:0.03782\n",
      "[260]\teval-logloss:0.03782\n",
      "[261]\teval-logloss:0.03782\n",
      "[262]\teval-logloss:0.03782\n",
      "[263]\teval-logloss:0.03782\n",
      "[264]\teval-logloss:0.03782\n",
      "[265]\teval-logloss:0.03782\n",
      "[266]\teval-logloss:0.03782\n",
      "[267]\teval-logloss:0.03782\n",
      "[268]\teval-logloss:0.03781\n",
      "[269]\teval-logloss:0.03782\n",
      "[270]\teval-logloss:0.03782\n",
      "[271]\teval-logloss:0.03782\n",
      "[272]\teval-logloss:0.03782\n",
      "[273]\teval-logloss:0.03782\n",
      "[274]\teval-logloss:0.03782\n",
      "[275]\teval-logloss:0.03783\n",
      "[276]\teval-logloss:0.03783\n",
      "[277]\teval-logloss:0.03783\n",
      "[278]\teval-logloss:0.03783\n",
      "[279]\teval-logloss:0.03783\n",
      "[280]\teval-logloss:0.03783\n",
      "[281]\teval-logloss:0.03782\n",
      "[282]\teval-logloss:0.03782\n",
      "[283]\teval-logloss:0.03782\n",
      "[284]\teval-logloss:0.03782\n",
      "[285]\teval-logloss:0.03782\n",
      "[286]\teval-logloss:0.03782\n",
      "[287]\teval-logloss:0.03782\n",
      "[288]\teval-logloss:0.03782\n",
      "[289]\teval-logloss:0.03782\n",
      "[290]\teval-logloss:0.03782\n",
      "[291]\teval-logloss:0.03782\n",
      "[292]\teval-logloss:0.03782\n",
      "[293]\teval-logloss:0.03782\n",
      "[294]\teval-logloss:0.03782\n",
      "[295]\teval-logloss:0.03782\n",
      "[296]\teval-logloss:0.03782\n",
      "[297]\teval-logloss:0.03782\n",
      "[298]\teval-logloss:0.03782\n",
      "[299]\teval-logloss:0.03783\n",
      "[300]\teval-logloss:0.03783\n",
      "[301]\teval-logloss:0.03782\n",
      "[302]\teval-logloss:0.03782\n",
      "[303]\teval-logloss:0.03782\n",
      "[304]\teval-logloss:0.03782\n",
      "[305]\teval-logloss:0.03782\n",
      "[306]\teval-logloss:0.03782\n",
      "[307]\teval-logloss:0.03782\n",
      "[308]\teval-logloss:0.03782\n",
      "[309]\teval-logloss:0.03782\n",
      "[310]\teval-logloss:0.03782\n",
      "[311]\teval-logloss:0.03783\n",
      "[312]\teval-logloss:0.03782\n",
      "[313]\teval-logloss:0.03783\n",
      "[314]\teval-logloss:0.03783\n",
      "[315]\teval-logloss:0.03783\n",
      "[316]\teval-logloss:0.03783\n",
      "[317]\teval-logloss:0.03783\n",
      "[318]\teval-logloss:0.03783\n",
      "[319]\teval-logloss:0.03783\n",
      "[320]\teval-logloss:0.03783\n",
      "[321]\teval-logloss:0.03783\n",
      "[322]\teval-logloss:0.03784\n",
      "[323]\teval-logloss:0.03784\n",
      "[324]\teval-logloss:0.03784\n",
      "[325]\teval-logloss:0.03784\n",
      "[326]\teval-logloss:0.03784\n",
      "[327]\teval-logloss:0.03784\n",
      "[328]\teval-logloss:0.03784\n",
      "[329]\teval-logloss:0.03784\n",
      "[330]\teval-logloss:0.03784\n",
      "[331]\teval-logloss:0.03784\n",
      "[332]\teval-logloss:0.03784\n",
      "[333]\teval-logloss:0.03784\n",
      "[334]\teval-logloss:0.03784\n",
      "[335]\teval-logloss:0.03784\n",
      "[336]\teval-logloss:0.03784\n",
      "[337]\teval-logloss:0.03784\n",
      "[338]\teval-logloss:0.03784\n",
      "[339]\teval-logloss:0.03785\n",
      "[340]\teval-logloss:0.03785\n",
      "[341]\teval-logloss:0.03785\n",
      "[342]\teval-logloss:0.03785\n",
      "[343]\teval-logloss:0.03785\n",
      "[344]\teval-logloss:0.03785\n",
      "[345]\teval-logloss:0.03786\n",
      "[346]\teval-logloss:0.03786\n",
      "[347]\teval-logloss:0.03787\n",
      "[348]\teval-logloss:0.03787\n",
      "[349]\teval-logloss:0.03787\n",
      "[350]\teval-logloss:0.03787\n",
      "[351]\teval-logloss:0.03787\n",
      "[352]\teval-logloss:0.03787\n",
      "[353]\teval-logloss:0.03787\n",
      "[354]\teval-logloss:0.03788\n",
      "[355]\teval-logloss:0.03787\n",
      "[356]\teval-logloss:0.03788\n",
      "[357]\teval-logloss:0.03788\n",
      "[358]\teval-logloss:0.03788\n",
      "[359]\teval-logloss:0.03788\n",
      "[360]\teval-logloss:0.03787\n",
      "[361]\teval-logloss:0.03787\n",
      "[362]\teval-logloss:0.03788\n",
      "[363]\teval-logloss:0.03788\n",
      "[364]\teval-logloss:0.03788\n",
      "[365]\teval-logloss:0.03788\n",
      "[366]\teval-logloss:0.03788\n",
      "[367]\teval-logloss:0.03788\n",
      "[368]\teval-logloss:0.03788\n",
      "[369]\teval-logloss:0.03788\n",
      "[370]\teval-logloss:0.03788\n",
      "[371]\teval-logloss:0.03788\n",
      "[372]\teval-logloss:0.03788\n",
      "[373]\teval-logloss:0.03788\n",
      "[374]\teval-logloss:0.03788\n",
      "[375]\teval-logloss:0.03788\n",
      "[376]\teval-logloss:0.03788\n",
      "[377]\teval-logloss:0.03788\n",
      "[378]\teval-logloss:0.03788\n",
      "[379]\teval-logloss:0.03788\n",
      "[380]\teval-logloss:0.03788\n",
      "[381]\teval-logloss:0.03788\n",
      "[382]\teval-logloss:0.03788\n",
      "[383]\teval-logloss:0.03788\n",
      "[384]\teval-logloss:0.03788\n",
      "[385]\teval-logloss:0.03788\n",
      "[386]\teval-logloss:0.03788\n",
      "[387]\teval-logloss:0.03787\n",
      "[388]\teval-logloss:0.03787\n",
      "[389]\teval-logloss:0.03787\n",
      "[390]\teval-logloss:0.03787\n",
      "[391]\teval-logloss:0.03788\n",
      "[392]\teval-logloss:0.03788\n",
      "[393]\teval-logloss:0.03788\n",
      "[394]\teval-logloss:0.03788\n",
      "[395]\teval-logloss:0.03788\n",
      "[396]\teval-logloss:0.03788\n",
      "[397]\teval-logloss:0.03788\n",
      "[398]\teval-logloss:0.03788\n",
      "[399]\teval-logloss:0.03788\n",
      "[400]\teval-logloss:0.03788\n",
      "[401]\teval-logloss:0.03788\n",
      "[402]\teval-logloss:0.03789\n",
      "[403]\teval-logloss:0.03789\n",
      "[404]\teval-logloss:0.03789\n",
      "[405]\teval-logloss:0.03788\n",
      "[406]\teval-logloss:0.03788\n",
      "[407]\teval-logloss:0.03788\n",
      "[408]\teval-logloss:0.03788\n",
      "[409]\teval-logloss:0.03789\n",
      "[410]\teval-logloss:0.03789\n",
      "[411]\teval-logloss:0.03789\n",
      "[412]\teval-logloss:0.03789\n",
      "[413]\teval-logloss:0.03789\n",
      "[414]\teval-logloss:0.03790\n",
      "[415]\teval-logloss:0.03790\n",
      "[416]\teval-logloss:0.03790\n",
      "[417]\teval-logloss:0.03789\n",
      "[418]\teval-logloss:0.03789\n",
      "[419]\teval-logloss:0.03790\n",
      "[420]\teval-logloss:0.03790\n",
      "[421]\teval-logloss:0.03790\n",
      "[422]\teval-logloss:0.03790\n",
      "[423]\teval-logloss:0.03790\n",
      "[424]\teval-logloss:0.03790\n",
      "[425]\teval-logloss:0.03790\n",
      "[426]\teval-logloss:0.03791\n",
      "[427]\teval-logloss:0.03791\n",
      "[428]\teval-logloss:0.03791\n",
      "[429]\teval-logloss:0.03791\n",
      "[430]\teval-logloss:0.03791\n",
      "[431]\teval-logloss:0.03791\n",
      "[432]\teval-logloss:0.03791\n",
      "[433]\teval-logloss:0.03791\n",
      "[434]\teval-logloss:0.03791\n",
      "[435]\teval-logloss:0.03791\n",
      "[436]\teval-logloss:0.03791\n",
      "[437]\teval-logloss:0.03791\n",
      "[438]\teval-logloss:0.03791\n",
      "[439]\teval-logloss:0.03791\n",
      "[440]\teval-logloss:0.03792\n",
      "[441]\teval-logloss:0.03792\n",
      "[442]\teval-logloss:0.03792\n",
      "[443]\teval-logloss:0.03792\n",
      "[444]\teval-logloss:0.03792\n",
      "[445]\teval-logloss:0.03792\n",
      "[446]\teval-logloss:0.03792\n",
      "[447]\teval-logloss:0.03792\n",
      "[448]\teval-logloss:0.03792\n",
      "[449]\teval-logloss:0.03792\n",
      "[450]\teval-logloss:0.03792\n",
      "[451]\teval-logloss:0.03792\n",
      "[452]\teval-logloss:0.03792\n",
      "[453]\teval-logloss:0.03792\n",
      "[454]\teval-logloss:0.03792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/05 00:01:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: poisonous-mushroom-classifier, version 4\n",
      "2024/08/05 00:01:38 INFO mlflow.tracking._tracking_service.client: 🏃 View run mean_strategy_1722790875 at: http://localhost:8080/#/experiments/901265626273758176/runs/3094f62eda8d4bda8e0e465077a4363c.\n",
      "2024/08/05 00:01:38 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/901265626273758176.\n",
      "2024/08/05 00:01:38 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/08/05 00:01:38 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisonous-mushroom-classifier\n",
      "poisonous-mushroom-classifier has been existed\n",
      "Name: poisonous-mushroom-classifier\n",
      "Version: 4\n",
      "Description: \n",
      "Status: READY\n",
      "Stage: None\n"
     ]
    }
   ],
   "source": [
    "# Mlflow\n",
    "model_name = \"poisonous-mushroom-classifier\"\n",
    "project_name = \"Binary Prediction of Poisonous Mushrooms\"\n",
    "exp_name = \"train-mushroom-classifier\"\n",
    "exp_desc = \"Training model to submit to Binary Prediction of Poisonous Mushrooms.\"\n",
    "mlf_client = Mlflow(model_name=model_name)\n",
    "mlf_client.get_or_create_exp(\n",
    "    project_name=project_name,\n",
    "    experiment_name=exp_name,\n",
    "    experiment_description=exp_desc,\n",
    ")\n",
    "curr_timestamp = int(datetime.now(timezone.utc).timestamp())\n",
    "\n",
    "artifact_path = \"model\"\n",
    "with mlflow.start_run(\n",
    "    run_name=f\"mean_strategy_{curr_timestamp}\",\n",
    "    tags={\"metric\": \"logloss\"},\n",
    "    description=None,\n",
    "    log_system_metrics=True,\n",
    ") as run:\n",
    "    try:\n",
    "        mlflow.log_artifact(local_path=f\"{study_name}.db\", artifact_path=artifact_path)\n",
    "    except:\n",
    "        pass\n",
    "    # Use tunned params\n",
    "    tunned_params = {\n",
    "        \"eta\": 0.1899871885683955,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_child_weight\": 2,\n",
    "        \"gamma\": 0.42570860420610934,\n",
    "        \"subsample\": 0.74464089552046,\n",
    "        \"colsample_bytree\": 0.6449797444444113,\n",
    "        \"lambda\": 0.4134501484785982,\n",
    "        \"alpha\": 6.520908679019516,\n",
    "    }\n",
    "\n",
    "    best_params.update(tunned_params)\n",
    "\n",
    "    print(\"Training best model...\")\n",
    "    evals_best_result = {}\n",
    "    # Create the full pipeline with the XGBoost model\n",
    "    if is_custom_metric:\n",
    "        print(\"Training with custom metric\")\n",
    "        model = xgb.train(\n",
    "            params=best_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=10000,\n",
    "            evals=[(dval, \"eval\")],\n",
    "            feval=fval,\n",
    "            evals_result=evals_best_result,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Training with original metric\")\n",
    "        model = xgb.train(\n",
    "            params=best_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=10000,\n",
    "            evals=[(dval, \"eval\")],\n",
    "            evals_result=evals_best_result,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "    mcc, _, validate_df = matthews_corrcoef_score(model, dval, y_val_binarized, lb)\n",
    "    # logs evaluation tables\n",
    "    mlflow.log_table(data=validate_df, artifact_file=f\"eval_results_{today}.json\")\n",
    "\n",
    "    # logs metrics\n",
    "    metrics = {}\n",
    "    metrics[\"MCC\"] = mcc\n",
    "    metrics[metric] = evals_best_result[\"eval\"][metric][-1]\n",
    "    mlflow.log_metrics(metrics)\n",
    "    _, classes, _ = matthews_corrcoef_score(model, dtest, None, lb)\n",
    "    signature = infer_signature(X_test_transformed, classes)\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=artifact_path,\n",
    "        signature=signature,\n",
    "    )\n",
    "    # Register model name in the model registry\n",
    "    try:\n",
    "        mlf_client.register_model()\n",
    "    except:\n",
    "        print(f\"model {model_name} already registered\")\n",
    "    # Model versioning\n",
    "    mv = mlf_client.version_model(run.info.run_id)\n",
    "    print(f\"Name: {mv.name}\")\n",
    "    print(f\"Version: {mv.version}\")\n",
    "    print(f\"Description: {mv.description}\")\n",
    "    print(f\"Status: {mv.status}\")\n",
    "    print(f\"Stage: {mv.current_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_value</th>\n",
       "      <th>predicted_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623384</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623385</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623386</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623387</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623388</th>\n",
       "      <td>p</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623389 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual_value predicted_value\n",
       "0                 e               e\n",
       "1                 e               e\n",
       "2                 e               e\n",
       "3                 e               e\n",
       "4                 e               e\n",
       "...             ...             ...\n",
       "623384            e               e\n",
       "623385            e               e\n",
       "623386            e               e\n",
       "623387            e               e\n",
       "623388            p               p\n",
       "\n",
       "[623389 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation coefficient: 0.9833906875450024\n"
     ]
    }
   ],
   "source": [
    "mcc, _, validate_df = matthews_corrcoef_score(model, dval, y_val_binarized, lb)\n",
    "print(f\"correlation coefficient: {mcc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_test_transformed)\n\u001b[0;32m----> 2\u001b[0m _, classes \u001b[38;5;241m=\u001b[39m matthews_corrcoef_score(model, dtest, \u001b[38;5;28;01mNone\u001b[39;00m, lb)\n\u001b[1;32m      4\u001b[0m submit_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      5\u001b[0m submit_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "dtest = xgb.DMatrix(X_test_transformed)\n",
    "_, classes = matthews_corrcoef_score(model, dtest, None, lb)\n",
    "\n",
    "submit_df = pd.DataFrame()\n",
    "submit_df[\"id\"] = test[\"id\"]\n",
    "submit_df[\"class\"] = classes\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "feature_important = model.get_score(importance_type=\"gain\")\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(\n",
    "    by=\"score\", ascending=False\n",
    ")\n",
    "data.nlargest(40, columns=\"score\").plot(\n",
    "    kind=\"barh\", figsize=(20, 10)\n",
    ")  ## plot top 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
